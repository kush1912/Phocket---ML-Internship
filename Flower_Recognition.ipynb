{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Flower_Recognition.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kush1912/Phocket---ML-Internship/blob/master/Flower_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKFOFl2a2EM1",
        "colab_type": "code",
        "outputId": "937eae45-5bd9-4647-862e-499a377bd3b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "\n",
        "from keras.layers import Convolution2D\n",
        "\n",
        "from keras.layers import MaxPooling2D,AveragePooling2D\n",
        "\n",
        "from keras.layers import Flatten\n",
        "\n",
        "from keras.layers import Dense\n",
        "\n",
        "from keras.layers import Activation\n",
        "\n",
        "from keras.layers import Dropout\n",
        "\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "from keras.layers.normalization import BatchNormalization"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NshfxPOW2XgB",
        "colab_type": "code",
        "outputId": "411a6e67-e5e0-4942-f005-29b4cf707a25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "# Initialising the CNN\n",
        "model = Sequential()\n",
        "\n",
        "# 1 - Convolution\n",
        "model.add(Convolution2D(64,(3,3), border_mode='same', input_shape=(48, 48, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# 2nd Convolution layer\n",
        "model.add(Convolution2D(128,(5,5), border_mode='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "\n",
        "# 3rd Convolution layer\n",
        "model.add(Convolution2D(512,(3,3), border_mode='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "\n",
        "# Flattening\n",
        "model.add(Flatten())\n",
        "\n",
        "# Fully connected layer 1st layer\n",
        "model.add(Dense(256))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "\n",
        "# Fully connected layer 2nd layer\n",
        "model.add(Dense(512))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Dense(5, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), input_shape=(48, 48, 3..., padding=\"same\")`\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (5, 5), padding=\"same\")`\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), padding=\"same\")`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWNvUV-D2ais",
        "colab_type": "code",
        "outputId": "1e2b4cb6-3a02-4c85-ca7b-b5f12de6a59a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from zipfile import ZipFile\n",
        "file_name='TESTING.zip'\n",
        "\n",
        "with ZipFile(file_name,'r') as zip:\n",
        "    zip.extractall()\n",
        "    print('Done')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qEBJEcg2cl2",
        "colab_type": "code",
        "outputId": "e5b51c4b-09f6-466b-8ebb-ab401c3e12b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from zipfile import ZipFile\n",
        "file_name='Class.zip'\n",
        "\n",
        "with ZipFile(file_name,'r') as zip:\n",
        "    zip.extractall()\n",
        "    print('Done')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIuK06Ld2fNv",
        "colab_type": "code",
        "outputId": "e42f71b9-da91-4819-8624-dbc421972d52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5851
        }
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen =  ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "\n",
        "test_datagen =  ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "training_set = train_datagen.flow_from_directory('Class/TRAINING',\n",
        "                                                 target_size=(48, 48),\n",
        "                                                 batch_size=64,\n",
        "                                                 class_mode='categorical',color_mode='rgb')\n",
        "\n",
        "test_set = test_datagen.flow_from_directory('Class/VALIDATION',\n",
        "                                             target_size=(48, 48),\n",
        "                                             batch_size=64,\n",
        "                                             class_mode='categorical',color_mode='rgb')\n",
        "\n",
        "#Save partly trained model# save best weights\n",
        "checkpointer = ModelCheckpoint(filepath='Flower.h5', verbose=1, save_best_only=True)\n",
        "\n",
        "\n",
        "model.fit_generator(training_set,\n",
        "                         steps_per_epoch=3683/100,\n",
        "                         epochs=50,\n",
        "                         validation_data=test_set,callbacks=[checkpointer],\n",
        "                         validation_steps=759/100,verbose=1)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 3377 images belonging to 5 classes.\n",
            "Found 473 images belonging to 5 classes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/70\n",
            "37/36 [==============================] - 22s 598ms/step - loss: 1.2915 - acc: 0.4472 - val_loss: 1.6374 - val_acc: 0.4443\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.63744, saving model to Flower.h5\n",
            "Epoch 2/70\n",
            "37/36 [==============================] - 16s 431ms/step - loss: 1.1223 - acc: 0.5416 - val_loss: 1.6326 - val_acc: 0.4287\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.63744 to 1.63259, saving model to Flower.h5\n",
            "Epoch 3/70\n",
            "37/36 [==============================] - 16s 432ms/step - loss: 1.0959 - acc: 0.5614 - val_loss: 1.3465 - val_acc: 0.4735\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.63259 to 1.34651, saving model to Flower.h5\n",
            "Epoch 4/70\n",
            "37/36 [==============================] - 16s 432ms/step - loss: 0.9858 - acc: 0.6047 - val_loss: 1.1841 - val_acc: 0.5679\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.34651 to 1.18405, saving model to Flower.h5\n",
            "Epoch 5/70\n",
            "37/36 [==============================] - 16s 436ms/step - loss: 0.9501 - acc: 0.6245 - val_loss: 1.4019 - val_acc: 0.4673\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 1.18405\n",
            "Epoch 6/70\n",
            "37/36 [==============================] - 16s 430ms/step - loss: 0.9032 - acc: 0.6480 - val_loss: 1.1386 - val_acc: 0.5610\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.18405 to 1.13864, saving model to Flower.h5\n",
            "Epoch 7/70\n",
            "37/36 [==============================] - 16s 436ms/step - loss: 0.8662 - acc: 0.6575 - val_loss: 1.4151 - val_acc: 0.5258\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 1.13864\n",
            "Epoch 8/70\n",
            "37/36 [==============================] - 16s 429ms/step - loss: 0.8008 - acc: 0.6747 - val_loss: 1.5162 - val_acc: 0.5219\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 1.13864\n",
            "Epoch 9/70\n",
            "37/36 [==============================] - 16s 430ms/step - loss: 0.7783 - acc: 0.6816 - val_loss: 1.5255 - val_acc: 0.5772\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 1.13864\n",
            "Epoch 10/70\n",
            "37/36 [==============================] - 16s 431ms/step - loss: 0.7788 - acc: 0.6917 - val_loss: 1.0681 - val_acc: 0.6200\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.13864 to 1.06810, saving model to Flower.h5\n",
            "Epoch 11/70\n",
            "37/36 [==============================] - 16s 437ms/step - loss: 0.7556 - acc: 0.7036 - val_loss: 1.2677 - val_acc: 0.6025\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 1.06810\n",
            "Epoch 12/70\n",
            "37/36 [==============================] - 16s 432ms/step - loss: 0.7440 - acc: 0.7159 - val_loss: 1.3312 - val_acc: 0.6068\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 1.06810\n",
            "Epoch 13/70\n",
            "37/36 [==============================] - 16s 431ms/step - loss: 0.6927 - acc: 0.7277 - val_loss: 1.1268 - val_acc: 0.6694\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 1.06810\n",
            "Epoch 14/70\n",
            "37/36 [==============================] - 16s 432ms/step - loss: 0.6801 - acc: 0.7314 - val_loss: 1.1556 - val_acc: 0.5961\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 1.06810\n",
            "Epoch 15/70\n",
            "37/36 [==============================] - 16s 437ms/step - loss: 0.7045 - acc: 0.7197 - val_loss: 0.9015 - val_acc: 0.6831\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.06810 to 0.90155, saving model to Flower.h5\n",
            "Epoch 16/70\n",
            "37/36 [==============================] - 16s 428ms/step - loss: 0.6447 - acc: 0.7546 - val_loss: 0.8780 - val_acc: 0.6882\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.90155 to 0.87800, saving model to Flower.h5\n",
            "Epoch 17/70\n",
            "37/36 [==============================] - 16s 438ms/step - loss: 0.6341 - acc: 0.7584 - val_loss: 1.3637 - val_acc: 0.5515\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.87800\n",
            "Epoch 18/70\n",
            "37/36 [==============================] - 16s 432ms/step - loss: 0.6271 - acc: 0.7541 - val_loss: 1.5762 - val_acc: 0.5268\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.87800\n",
            "Epoch 19/70\n",
            "37/36 [==============================] - 16s 431ms/step - loss: 0.5900 - acc: 0.7663 - val_loss: 1.1422 - val_acc: 0.6623\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.87800\n",
            "Epoch 20/70\n",
            "37/36 [==============================] - 16s 430ms/step - loss: 0.5800 - acc: 0.7753 - val_loss: 0.8816 - val_acc: 0.7034\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.87800\n",
            "Epoch 21/70\n",
            "37/36 [==============================] - 16s 433ms/step - loss: 0.5396 - acc: 0.7945 - val_loss: 1.1764 - val_acc: 0.6588\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.87800\n",
            "Epoch 22/70\n",
            "37/36 [==============================] - 16s 434ms/step - loss: 0.5403 - acc: 0.7873 - val_loss: 1.8533 - val_acc: 0.5277\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.87800\n",
            "Epoch 23/70\n",
            "37/36 [==============================] - 16s 431ms/step - loss: 0.5358 - acc: 0.7993 - val_loss: 0.9732 - val_acc: 0.6552\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.87800\n",
            "Epoch 24/70\n",
            "37/36 [==============================] - 16s 429ms/step - loss: 0.5244 - acc: 0.7981 - val_loss: 1.4080 - val_acc: 0.5907\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.87800\n",
            "Epoch 25/70\n",
            "37/36 [==============================] - 16s 431ms/step - loss: 0.4900 - acc: 0.8106 - val_loss: 0.8350 - val_acc: 0.7139\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.87800 to 0.83499, saving model to Flower.h5\n",
            "Epoch 26/70\n",
            "37/36 [==============================] - 16s 430ms/step - loss: 0.5247 - acc: 0.8101 - val_loss: 1.4200 - val_acc: 0.5992\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.83499\n",
            "Epoch 27/70\n",
            "37/36 [==============================] - 16s 432ms/step - loss: 0.4840 - acc: 0.8146 - val_loss: 0.9237 - val_acc: 0.7031\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.83499\n",
            "Epoch 28/70\n",
            "37/36 [==============================] - 16s 430ms/step - loss: 0.4600 - acc: 0.8281 - val_loss: 1.1386 - val_acc: 0.6267\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.83499\n",
            "Epoch 29/70\n",
            "37/36 [==============================] - 16s 430ms/step - loss: 0.4662 - acc: 0.8169 - val_loss: 1.1735 - val_acc: 0.6253\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.83499\n",
            "Epoch 30/70\n",
            "37/36 [==============================] - 16s 433ms/step - loss: 0.4463 - acc: 0.8357 - val_loss: 1.2716 - val_acc: 0.6493\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.83499\n",
            "Epoch 31/70\n",
            "37/36 [==============================] - 16s 429ms/step - loss: 0.4059 - acc: 0.8507 - val_loss: 1.0604 - val_acc: 0.6989\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.83499\n",
            "Epoch 32/70\n",
            "37/36 [==============================] - 16s 433ms/step - loss: 0.4288 - acc: 0.8292 - val_loss: 1.1014 - val_acc: 0.6151\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.83499\n",
            "Epoch 33/70\n",
            "37/36 [==============================] - 16s 426ms/step - loss: 0.4076 - acc: 0.8485 - val_loss: 1.2656 - val_acc: 0.6264\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.83499\n",
            "Epoch 34/70\n",
            "37/36 [==============================] - 16s 430ms/step - loss: 0.3842 - acc: 0.8522 - val_loss: 0.9156 - val_acc: 0.7211\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.83499\n",
            "Epoch 35/70\n",
            "37/36 [==============================] - 16s 431ms/step - loss: 0.4016 - acc: 0.8447 - val_loss: 0.9763 - val_acc: 0.6816\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.83499\n",
            "Epoch 36/70\n",
            "37/36 [==============================] - 16s 428ms/step - loss: 0.3816 - acc: 0.8579 - val_loss: 1.5320 - val_acc: 0.6030\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.83499\n",
            "Epoch 37/70\n",
            "37/36 [==============================] - 16s 432ms/step - loss: 0.3586 - acc: 0.8704 - val_loss: 1.8079 - val_acc: 0.5655\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.83499\n",
            "Epoch 38/70\n",
            "37/36 [==============================] - 16s 430ms/step - loss: 0.3281 - acc: 0.8718 - val_loss: 1.2238 - val_acc: 0.6616\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.83499\n",
            "Epoch 39/70\n",
            "37/36 [==============================] - 16s 428ms/step - loss: 0.3574 - acc: 0.8665 - val_loss: 1.0091 - val_acc: 0.7122\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.83499\n",
            "Epoch 40/70\n",
            "37/36 [==============================] - 16s 429ms/step - loss: 0.3841 - acc: 0.8602 - val_loss: 1.2318 - val_acc: 0.6752\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.83499\n",
            "Epoch 41/70\n",
            "37/36 [==============================] - 16s 429ms/step - loss: 0.3199 - acc: 0.8845 - val_loss: 1.1452 - val_acc: 0.6672\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.83499\n",
            "Epoch 42/70\n",
            "37/36 [==============================] - 16s 429ms/step - loss: 0.3145 - acc: 0.8768 - val_loss: 1.3483 - val_acc: 0.6538\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.83499\n",
            "Epoch 43/70\n",
            "37/36 [==============================] - 16s 430ms/step - loss: 0.3437 - acc: 0.8691 - val_loss: 1.2241 - val_acc: 0.6565\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.83499\n",
            "Epoch 44/70\n",
            "37/36 [==============================] - 16s 429ms/step - loss: 0.3326 - acc: 0.8758 - val_loss: 1.2265 - val_acc: 0.6325\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.83499\n",
            "Epoch 45/70\n",
            "37/36 [==============================] - 16s 429ms/step - loss: 0.2890 - acc: 0.8868 - val_loss: 1.1796 - val_acc: 0.6938\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.83499\n",
            "Epoch 46/70\n",
            "37/36 [==============================] - 16s 424ms/step - loss: 0.2950 - acc: 0.8915 - val_loss: 1.0186 - val_acc: 0.7021\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.83499\n",
            "Epoch 47/70\n",
            "37/36 [==============================] - 16s 428ms/step - loss: 0.2778 - acc: 0.8948 - val_loss: 1.2197 - val_acc: 0.6674\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.83499\n",
            "Epoch 48/70\n",
            "37/36 [==============================] - 16s 426ms/step - loss: 0.2858 - acc: 0.8956 - val_loss: 1.2377 - val_acc: 0.6616\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.83499\n",
            "Epoch 49/70\n",
            "37/36 [==============================] - 16s 431ms/step - loss: 0.2490 - acc: 0.9099 - val_loss: 0.9870 - val_acc: 0.7308\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.83499\n",
            "Epoch 50/70\n",
            "37/36 [==============================] - 16s 427ms/step - loss: 0.2559 - acc: 0.9050 - val_loss: 1.5311 - val_acc: 0.6281\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.83499\n",
            "Epoch 51/70\n",
            "37/36 [==============================] - 16s 427ms/step - loss: 0.2723 - acc: 0.8915 - val_loss: 1.2219 - val_acc: 0.7011\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.83499\n",
            "Epoch 52/70\n",
            "37/36 [==============================] - 16s 427ms/step - loss: 0.2620 - acc: 0.9051 - val_loss: 1.6040 - val_acc: 0.6211\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.83499\n",
            "Epoch 53/70\n",
            "37/36 [==============================] - 16s 433ms/step - loss: 0.2383 - acc: 0.9096 - val_loss: 1.0958 - val_acc: 0.7146\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.83499\n",
            "Epoch 54/70\n",
            "37/36 [==============================] - 16s 432ms/step - loss: 0.2195 - acc: 0.9223 - val_loss: 1.0101 - val_acc: 0.7356\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.83499\n",
            "Epoch 55/70\n",
            "37/36 [==============================] - 16s 428ms/step - loss: 0.2630 - acc: 0.9053 - val_loss: 1.1981 - val_acc: 0.6809\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.83499\n",
            "Epoch 56/70\n",
            "37/36 [==============================] - 16s 422ms/step - loss: 0.2441 - acc: 0.9056 - val_loss: 1.3071 - val_acc: 0.6690\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.83499\n",
            "Epoch 57/70\n",
            "37/36 [==============================] - 16s 430ms/step - loss: 0.2154 - acc: 0.9189 - val_loss: 1.5602 - val_acc: 0.6377\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.83499\n",
            "Epoch 58/70\n",
            "37/36 [==============================] - 16s 430ms/step - loss: 0.2211 - acc: 0.9191 - val_loss: 1.3169 - val_acc: 0.6560\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.83499\n",
            "Epoch 59/70\n",
            "37/36 [==============================] - 16s 427ms/step - loss: 0.2147 - acc: 0.9184 - val_loss: 1.4177 - val_acc: 0.6641\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.83499\n",
            "Epoch 60/70\n",
            "37/36 [==============================] - 16s 424ms/step - loss: 0.2012 - acc: 0.9345 - val_loss: 1.5925 - val_acc: 0.6218\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.83499\n",
            "Epoch 61/70\n",
            "37/36 [==============================] - 16s 428ms/step - loss: 0.1907 - acc: 0.9266 - val_loss: 1.3027 - val_acc: 0.6767\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.83499\n",
            "Epoch 62/70\n",
            "37/36 [==============================] - 16s 424ms/step - loss: 0.2119 - acc: 0.9230 - val_loss: 1.5122 - val_acc: 0.6455\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.83499\n",
            "Epoch 63/70\n",
            "37/36 [==============================] - 16s 432ms/step - loss: 0.2060 - acc: 0.9219 - val_loss: 1.1623 - val_acc: 0.7035\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.83499\n",
            "Epoch 64/70\n",
            "37/36 [==============================] - 16s 419ms/step - loss: 0.1989 - acc: 0.9270 - val_loss: 1.3417 - val_acc: 0.6672\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.83499\n",
            "Epoch 65/70\n",
            "37/36 [==============================] - 16s 423ms/step - loss: 0.1804 - acc: 0.9290 - val_loss: 1.2350 - val_acc: 0.6847\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.83499\n",
            "Epoch 66/70\n",
            "37/36 [==============================] - 16s 420ms/step - loss: 0.1775 - acc: 0.9283 - val_loss: 1.2599 - val_acc: 0.6907\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.83499\n",
            "Epoch 67/70\n",
            "37/36 [==============================] - 16s 426ms/step - loss: 0.1893 - acc: 0.9341 - val_loss: 1.1443 - val_acc: 0.7283\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.83499\n",
            "Epoch 68/70\n",
            "37/36 [==============================] - 16s 428ms/step - loss: 0.1721 - acc: 0.9376 - val_loss: 1.3182 - val_acc: 0.6808\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.83499\n",
            "Epoch 69/70\n",
            "37/36 [==============================] - 16s 434ms/step - loss: 0.1631 - acc: 0.9440 - val_loss: 1.1264 - val_acc: 0.7104\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.83499\n",
            "Epoch 70/70\n",
            "37/36 [==============================] - 16s 421ms/step - loss: 0.1679 - acc: 0.9350 - val_loss: 1.3188 - val_acc: 0.6864\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.83499\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 48, 48, 64)        1792      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 48, 48, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 48, 48, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 24, 24, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 24, 24, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 24, 24, 128)       204928    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 24, 24, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 12, 12, 512)       590336    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 12, 12, 512)       2048      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 12, 12, 512)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 512)         0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 6, 6, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 18432)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               4718848   \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 5)                 2565      \n",
            "=================================================================\n",
            "Total params: 5,655,941\n",
            "Trainable params: 5,652,997\n",
            "Non-trainable params: 2,944\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRvd_6_ndk4h",
        "colab_type": "code",
        "outputId": "54f7949f-9bca-4680-a2ce-336fe8d084b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#trained model evaluation \n",
        "test1_set = test_datagen.flow_from_directory('TESTING',\n",
        "                                             target_size=(48, 48),\n",
        "                                             batch_size=32,\n",
        "                                             class_mode ='categorical',color_mode='rgb')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 473 images belonging to 5 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XhhmybEeQFo",
        "colab_type": "code",
        "outputId": "022a35e2-d973-4a2d-ddee-fb65626fe885",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.evaluate_generator(generator=test1_set,steps=32)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.1177664948498258, 0.7257425749655997]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4EiPzeDeSfx",
        "colab_type": "code",
        "outputId": "4791beca-962b-4453-9bf6-903b9cac7b55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "generator = test_datagen.flow_from_directory(\n",
        "        'TESTING',\n",
        "        target_size=(48, 48),\n",
        "        batch_size=1,\n",
        "        class_mode=None,  # only data, no labels\n",
        "        shuffle=False,color_mode='rgb')  # keep data in same order as labels\n",
        "\n",
        "probabilities = model.predict_generator(generator, 473)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 473 images belonging to 5 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXLcEdrgeVGI",
        "colab_type": "code",
        "outputId": "50dc0de8-7cae-429c-cdf0-1346b85a9a65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "probabilities"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.92563736e-02, 2.03005075e-01, 1.11590505e-01, 3.28034163e-04,\n",
              "        1.62736289e-02],\n",
              "       [1.14202499e-04, 6.05583191e-05, 9.13884521e-01, 2.08616257e-07,\n",
              "        3.31308553e-03],\n",
              "       [9.73218441e-01, 0.00000000e+00, 1.40070915e-06, 4.47034836e-07,\n",
              "        1.08030304e-06],\n",
              "       ...,\n",
              "       [7.87377357e-05, 2.32738256e-03, 9.01190639e-02, 1.96993351e-05,\n",
              "        7.76761174e-01],\n",
              "       [3.33786011e-05, 0.00000000e+00, 5.15580177e-06, 2.98023224e-08,\n",
              "        9.99661684e-01],\n",
              "       [2.68220901e-07, 4.79519367e-05, 3.25143337e-05, 2.85327435e-04,\n",
              "        9.41879034e-01]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ol3N4Omvp1kw",
        "colab_type": "code",
        "outputId": "d59ef10d-d9b2-405a-b4c3-5fd3e0ab4880",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pba = [i * 10 for i in probabilities]\n",
        "type(pba)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtu6m-F2p-s-",
        "colab_type": "code",
        "outputId": "0ea5063f-9ec4-487b-c12f-30733be2e003",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "pp = np.asarray(pba)\n",
        "type(pp)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIzHi5TqqBYL",
        "colab_type": "code",
        "outputId": "196b41d6-0221-42ba-f99b-292019781597",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "po = pp.astype(int)\n",
        "type(po)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "samYKJabqTzj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Funtions to print confusion matrix\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score,f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sb\n",
        "import pandas as pd\n",
        "\n",
        "def confusion_plot_desc(cm):\n",
        "    labels = ['Daisy','Dandelion','Rose','Sunflower','Tulip']\n",
        "    df_cm = pd.DataFrame(cm,index=[i for i in labels],columns = [i for i in labels])\n",
        "    plt.figure(figsize=(10,10))\n",
        "    #sb.set(font_scale=1.5)\n",
        "    ax = plt.subplot()\n",
        "    sb.heatmap(df_cm, annot=True, ax=ax, annot_kws={\"size\" :16}, cmap=\"Greens\"); #annot=True to annotate cells\n",
        "    \n",
        "    # labels, title and ticks\n",
        "    ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels') \n",
        "    ax.set_title('Confusion Matrix')\n",
        "    #ax.xaxis.set_ticklabels(labels); ax.yaxis.set_ticklabels(labels)\n",
        "   \n",
        "    \n",
        "# function to print Description report\n",
        "def print_Desc_report(y_true, y_pred):\n",
        "    print('Testing Accuracy Of Description: {}'.format(accuracy_score(y_true, y_pred)))\n",
        "    print('Testing F1 score Of Description: {}'.format(f1_score(y_true, y_pred, average='weighted')))\n",
        "    print(metrics.classification_report(y_true, y_pred))\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    confusion_plot_desc(cm)\n",
        "    print()\n",
        "\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyjrGpvPqo-n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "y_true = np.array([0] * 88 + [1] * 110 + [2] * 88 + [3] * 77 + [4] * 110)\n",
        "y = [0,1,2,3,4]\n",
        "y_pred =  po.argmax(axis = 1)\n",
        "#print(y_pred)\n",
        "#for i in range(0,len(y_pred)):\n",
        " # if(y_pred[i] > 0.5):\n",
        "  #   y_pred[i] = 1\n",
        "#print(y_pred)\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "#print(cm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xS6MEa0oBqZ2",
        "colab_type": "code",
        "outputId": "9b988061-f20f-4596-a38d-0ca611542d4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 876
        }
      },
      "source": [
        "print_Desc_report(y_true, y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing Accuracy Of Description: 0.7061310782241015\n",
            "Testing F1 score Of Description: 0.7055202397090501\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.91      0.67        88\n",
            "           1       0.92      0.51      0.65       110\n",
            "           2       0.67      0.77      0.72        88\n",
            "           3       0.82      0.83      0.83        77\n",
            "           4       0.82      0.60      0.69       110\n",
            "\n",
            "    accuracy                           0.71       473\n",
            "   macro avg       0.75      0.72      0.71       473\n",
            "weighted avg       0.76      0.71      0.71       473\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjYAAAJcCAYAAADq2e4JAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiIiEhkKbERERCQyFNiIiIhIZPw/W5MMFvCBSuEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x720 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}